# -*- coding: utf-8 -*-
"""Main proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZVQh1sXEr0KbSIsVvqzJtewAC7vrlj_e
"""

# Install required libraries
!pip install kagglehub[pandas-datasets]
!pip install torch torchvision torchaudio
!pip install timm
!pip install opencv-python
!pip install scikit-learn
!pip install matplotlib seaborn
!pip install grad-cam
!pip install Pillow
!pip install numpy pandas
!pip install tqdm

import kagglehub
from kagglehub import KaggleDatasetAdapter
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import timm
from PIL import Image
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, roc_auc_score
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

import kagglehub
import os
from pathlib import Path
import pandas as pd

# Download the dataset
print("Downloading dataset...")
dataset_path = kagglehub.dataset_download("hamza000ali/lung-cancer-dataset-cv-4786-images")

print(f"\nDataset path: {dataset_path}")

# Find CSV file
csv_files = []
for root, dirs, files in os.walk(dataset_path):
    for file in files:
        if file.endswith('.csv'):
            csv_files.append(os.path.join(root, file))

if csv_files:
    print(f"\nFound CSV file(s): {csv_files}")

    # Load the first CSV
    df = pd.read_csv(csv_files[0])
    print(f"\nCSV shape: {df.shape}")
    print(f"\nColumns: {df.columns.tolist()}")
    print(f"\nFirst few rows:")
    print(df.head())

    # Check dataset structure
    print(f"\n{'='*60}")
    print("ANALYZING DATASET STRUCTURE...")
    print(f"{'='*60}")

    # Find all image files in the dataset
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']
    image_files_dict = {}

    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if any(file.lower().endswith(ext) for ext in image_extensions):
                full_path = os.path.join(root, file)
                # Store with filename as key
                image_files_dict[file] = full_path
                # Also store with full path
                image_files_dict[full_path] = full_path

    print(f"\nFound {len(image_files_dict)} image files")

    # Determine dataset type based on CSV columns
    if 'filename' in df.columns:
        print("\nDataset type: Image classification with filename column")
        # Original structure - filename based
        label_columns = [col for col in df.columns if col != 'filename']

        # Create binary labels
        df['label'] = 0
        df['label_name'] = 'Normal'

        if 'normal' in df.columns or ' normal' in df.columns:
            normal_col = 'normal' if 'normal' in df.columns else ' normal'
            for idx, row in df.iterrows():
                is_normal = row[normal_col] == 1
                is_cancer = any(row[col] == 1 for col in label_columns if col != normal_col)

                if is_cancer:
                    df.at[idx, 'label'] = 1
                    df.at[idx, 'label_name'] = 'Cancer'
                elif is_normal:
                    df.at[idx, 'label'] = 0
                    df.at[idx, 'label_name'] = 'Normal'

        # Build full image paths
        df['image_path'] = df['filename'].apply(lambda fname: image_files_dict.get(fname, None))

    elif 'seriesuid' in df.columns or 'coordX' in df.columns:
        print("\nDataset type: LUNA-style nodule detection dataset")
        print("Note: This dataset has 3D coordinates. Creating image-based structure...")

        # This is a different type of dataset - we need to find actual image files
        # and create a new structure
        if len(image_files_dict) > 0:
            # Create a new dataframe from image files
            image_data = []
            for filename, img_path in image_files_dict.items():
                # Try to infer label from filename or path
                path_lower = str(img_path).lower()
                label = None
                label_name = 'Unknown'

                if any(word in path_lower for word in ['cancer', 'malignant', 'positive', 'tumor', 'nodule']):
                    label = 1
                    label_name = 'Cancer'
                elif any(word in path_lower for word in ['normal', 'benign', 'negative', 'healthy']):
                    label = 0
                    label_name = 'Normal'

                image_data.append({
                    'filename': os.path.basename(img_path),
                    'image_path': img_path,
                    'label': label,
                    'label_name': label_name
                })

            df = pd.DataFrame(image_data)
            print(f"\nCreated image dataset with {len(df)} images")
        else:
            print("\nWARNING: No image files found in dataset!")
            print("This dataset may require special handling.")
            df = None

    else:
        # Try to find image column or create from image files
        print("\nDataset type: Unknown structure - attempting to adapt...")

        # Look for any column that might contain image paths
        potential_image_cols = [col for col in df.columns if any(word in col.lower()
                          for word in ['image', 'file', 'path', 'img', 'photo'])]

        if potential_image_cols:
            print(f"Found potential image columns: {potential_image_cols}")
            image_col = potential_image_cols[0]
            df['image_path'] = df[image_col]
        elif len(image_files_dict) > 0:
            # Create from image files
            image_data = []
            for filename, img_path in image_files_dict.items():
                path_lower = str(img_path).lower()
                label = None
                label_name = 'Unknown'

                if any(word in path_lower for word in ['cancer', 'malignant', 'positive']):
                    label = 1
                    label_name = 'Cancer'
                elif any(word in path_lower for word in ['normal', 'benign', 'negative']):
                    label = 0
                    label_name = 'Normal'

                image_data.append({
                    'filename': os.path.basename(img_path),
                    'image_path': img_path,
                    'label': label,
                    'label_name': label_name
                })

            df = pd.DataFrame(image_data)
        else:
            print("Could not determine dataset structure. Please check manually.")
            df = None

    # Final processing if df exists
    if df is not None and 'image_path' in df.columns:
        # Check if images exist
        df['image_exists'] = df['image_path'].apply(lambda path: os.path.exists(path) if path and isinstance(path, str) else False)

        print(f"\n{'='*60}")
        print("DATASET PROCESSING COMPLETE")
        print(f"{'='*60}")
        print(f"Total rows: {len(df)}")
        print(f"Images found: {df['image_exists'].sum() if 'image_exists' in df.columns else 'N/A'}")

        if 'label' in df.columns:
            print(f"\nLabel distribution:")
            print(df['label_name'].value_counts() if 'label_name' in df.columns else df['label'].value_counts())
    else:
        print("\nERROR: Could not create proper dataset structure!")
        print("Please check the dataset manually or use a different dataset.")

    # Store for later use
    DATASET_ROOT = dataset_path

else:
    print("No CSV file found!")
    # Try to build dataset from image files only
    print("Attempting to build dataset from image files...")

    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']
    image_files = []

    for root, dirs, files in os.walk(dataset_path):
        for file in files:
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_files.append(os.path.join(root, file))

    if image_files:
        print(f"Found {len(image_files)} image files")
        # Create dataframe from images
        data = []
        for img_path in image_files:
            rel_path = os.path.relpath(img_path, dataset_path)
            path_parts = rel_path.split(os.sep)

            path_str = ' '.join(path_parts).lower()
            label = None
            label_name = 'Unknown'

            if any(word in path_str for word in ['cancer', 'malignant', 'positive', 'tumor']):
                label = 1
                label_name = 'Cancer'
            elif any(word in path_str for word in ['normal', 'benign', 'negative', 'healthy']):
                label = 0
                label_name = 'Normal'

            data.append({
                'image_path': img_path,
                'relative_path': rel_path,
                'filename': os.path.basename(img_path),
                'label': label,
                'label_name': label_name
            })

        df = pd.DataFrame(data)
        df['image_exists'] = True
        DATASET_ROOT = dataset_path
        print(f"Created dataset with {len(df)} images")
    else:
        df = None
        print("No image files found!")

print(f"\n{'='*60}")
if df is not None:
    print("Dataset loaded successfully!")
    print(f"{'='*60}")
else:
    print("Dataset loading failed. Please check the dataset structure.")
    print(f"{'='*60}")

# Quick image count summary
print("="*60)
print("  IMAGE COUNT SUMMARY")
print("="*60)

if 'label_name' in df.columns:
    label_counts = df['label_name'].value_counts()

    print(f"\nðŸ“Š Total Images: {len(df)}")
    print(f"\nðŸ”´ Cancer Images: {label_counts.get('Cancer', 0)} ({label_counts.get('Cancer', 0)/len(df)*100:.1f}%)")
    print(f"ðŸŸ¢ Normal Images: {label_counts.get('Normal', 0)} ({label_counts.get('Normal', 0)/len(df)*100:.1f}%)")

    # Visual representation
    print(f"\nVisual representation:")
    cancer_count = label_counts.get('Cancer', 0)
    normal_count = label_counts.get('Normal', 0)
    total = len(df)

    cancer_bar = 'â–ˆ' * int(cancer_count / total * 50)
    normal_bar = 'â–ˆ' * int(normal_count / total * 50)

    print(f"Cancer:  {cancer_bar} {cancer_count}")
    print(f"Normal:  {normal_bar} {normal_count}")

elif 'label' in df.columns:
    label_counts = df['label'].value_counts()
    cancer_count = label_counts.get(1, 0)
    normal_count = label_counts.get(0, 0)

    print(f"\nðŸ“Š Total Images: {len(df)}")
    print(f"\nðŸ”´ Cancer Images: {cancer_count} ({cancer_count/len(df)*100:.1f}%)")
    print(f"ðŸŸ¢ Normal Images: {normal_count} ({normal_count/len(df)*100:.1f}%)")

print("="*60)

# Explore the loaded dataset
print("="*60)
print("DATASET EXPLORATION")
print("="*60)

print(f"\nDataset shape: {df.shape}")
print(f"\nColumns: {df.columns.tolist()}")
print(f"\nFirst few rows:")
print(df.head())

print(f"\nLabel distribution:")
if 'label' in df.columns:
    print(df['label'].value_counts())
if 'label_name' in df.columns:
    print(df['label_name'].value_counts())

# Check for missing labels
if 'label' in df.columns:
    missing_labels = df['label'].isna().sum()
    print(f"\nMissing labels: {missing_labels}")

# Check image paths
if 'image_path' in df.columns:
    print(f"\nImage path statistics:")
    print(f"  Total rows: {len(df)}")
    print(f"  Images found: {df['image_exists'].sum()}")
    print(f"  Images missing: {(~df['image_exists']).sum()}")

    # Display sample image paths
    print(f"\nSample image paths (first 5 existing):")
    existing_df = df[df['image_exists']].head(5)
    for i, row in existing_df.iterrows():
        print(f"  {i+1}. {row['filename']}")
        print(f"      Path: {row['image_path']}")
        print(f"      Label: {row['label_name']} ({row['label']})")
else:
    print("\nWarning: 'image_path' column not found!")
    print("Available columns:", df.columns.tolist())

# Show cancer type distribution if available
cancer_type_cols = [col for col in df.columns if 'carcinoma' in col.lower() or 'adenocarcinoma' in col.lower()]
if cancer_type_cols:
    print(f"\nCancer type columns found: {cancer_type_cols}")
    for col in cancer_type_cols:
        count = (df[col] == 1).sum()
        if count > 0:
            print(f"  {col}: {count} cases")

class LungCancerDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        label = self.labels[idx]

        # Load image
        try:
            if isinstance(image_path, str):
                image = Image.open(image_path).convert('RGB')
            else:
                image = image_path if isinstance(image_path, Image.Image) else Image.fromarray(image_path).convert('RGB')
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # Return a blank image if loading fails
            image = Image.new('RGB', (224, 224), color='black')

        # Apply transforms
        if self.transform:
            image = self.transform(image)

        return image, label

print("Dataset class defined successfully!")

# Define image transformations
# For training: augmentation + normalization
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.3),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# For validation/testing: only normalization
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

print("Transforms defined successfully!")

# Filter out rows where images don't exist and labels are missing
df_labeled = df[(df['image_exists'] == True) & (df['label'].notna())].copy()

print(f"Valid labeled images: {len(df_labeled)} / {len(df)}")

if len(df_labeled) == 0:
    print("ERROR: No valid labeled images found!")
    print("Please check the dataset structure.")
else:
    # Get image paths and labels
    image_paths = df_labeled['image_path'].values
    labels = df_labeled['label'].astype(int).values

    print(f"\nLabel distribution:")
    unique, counts = np.unique(labels, return_counts=True)
    for u, c in zip(unique, counts):
        label_name = 'Normal' if u == 0 else 'Cancer'
        print(f"  {label_name} (Label {u}): {c} images")

    # Check if we have enough samples for stratified split
    min_samples = min(counts) if len(counts) > 1 else counts[0]

    if min_samples < 2:
        print("\nWarning: Not enough samples for stratified split. Using random split.")
        stratify = None
    else:
        stratify = labels

    # Split data into train, validation, and test sets
    # First split: train (70%) and temp (30%)
    train_paths, temp_paths, train_labels, temp_labels = train_test_split(
        image_paths, labels, test_size=0.3, random_state=42, stratify=stratify
    )

    # Second split: validation (15%) and test (15%)
    if stratify is not None:
        temp_stratify = temp_labels
    else:
        temp_stratify = None

    val_paths, test_paths, val_labels, test_labels = train_test_split(
        temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_stratify
    )

    print(f"\nData split:")
    print(f"  Train: {len(train_paths)} images")
    print(f"  Validation: {len(val_paths)} images")
    print(f"  Test: {len(test_paths)} images")

    # Show label distribution in each split
    print(f"\nTrain label distribution:")
    unique_train, counts_train = np.unique(train_labels, return_counts=True)
    for u, c in zip(unique_train, counts_train):
        label_name = 'Normal' if u == 0 else 'Cancer'
        print(f"  {label_name}: {c}")

    # Create datasets
    train_dataset = LungCancerDataset(train_paths, train_labels, transform=train_transform)
    val_dataset = LungCancerDataset(val_paths, val_labels, transform=val_transform)
    test_dataset = LungCancerDataset(test_paths, test_labels, transform=val_transform)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

    print("\nâœ“ Data loaders created successfully!")
    print("You can now proceed to training (Cell 10).")

class VisionTransformerClassifier(nn.Module):
    def __init__(self, num_classes=2, pretrained=True):
        super(VisionTransformerClassifier, self).__init__()
        # Load pretrained ViT model
        self.vit = timm.create_model('vit_base_patch16_224', pretrained=pretrained)

        # Replace the classifier head for binary classification
        num_features = self.vit.head.in_features
        self.vit.head = nn.Linear(num_features, num_classes)

    def forward(self, x):
        return self.vit(x)

    def get_features(self, x):
        # Extract features before classification head
        x = self.vit.patch_embed(x)
        x = self.vit._pos_embed(x)
        x = self.vit.norm_pre(x)
        for blk in self.vit.blocks:
            x = blk(x)
        x = self.vit.norm(x)
        return x

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

model = VisionTransformerClassifier(num_classes=2, pretrained=True)
model = model.to(device)
print("Vision Transformer model initialized!")

def train_epoch(model, train_loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in tqdm(train_loader, desc="Training"):
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()
        optimizer.step()

        # Statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    return epoch_loss, epoch_acc

def validate_epoch(model, val_loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="Validating"):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    epoch_loss = running_loss / len(val_loader)
    epoch_acc = 100 * correct / total
    return epoch_loss, epoch_acc, all_preds, all_labels

print("Training functions defined!")

# Training parameters
num_epochs = 20
learning_rate = 1e-4
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Training history
train_losses = []
train_accs = []
val_losses = []
val_accs = []

print("Starting training...")
print(f"Number of epochs: {num_epochs}")
print(f"Learning rate: {learning_rate}")

for epoch in range(num_epochs):
    print(f"\nEpoch {epoch+1}/{num_epochs}")

    # Train
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    # Validate
    val_loss, val_acc, _, _ = validate_epoch(model, val_loader, criterion, device)
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    # Update learning rate
    scheduler.step()

    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

print("\nTraining completed!")

# Plot training history
def plot_training_history(train_losses, train_accs, val_losses, val_accs):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # Loss plot
    ax1.plot(train_losses, label='Train Loss', linewidth=2)
    ax1.plot(val_losses, label='Val Loss', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Accuracy plot
    ax2.plot(train_accs, label='Train Acc', linewidth=2)
    ax2.plot(val_accs, label='Val Acc', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

plot_training_history(train_losses, train_accs, val_losses, val_accs)

from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
from pytorch_grad_cam.utils.image import show_cam_on_image
from sklearn.metrics import roc_curve, auc, roc_auc_score
import torch.nn.functional as F

# Custom ViT Grad-CAM implementation
class ViTGradCAM:
    """
    Custom Grad-CAM for Vision Transformer using attention weights
    """
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.model.eval()

    def generate_cam(self, input_tensor, target_class=None):
        """
        Generate CAM using attention weights from ViT
        """
        input_tensor = input_tensor.to(self.device)

        with torch.no_grad():
            # Forward pass to get prediction
            output = self.model(input_tensor)
            if target_class is None:
                target_class = output.argmax(dim=1).item()

            # Get patch embeddings
            x = self.model.vit.patch_embed(input_tensor)
            x = self.model.vit._pos_embed(x)
            x = self.model.vit.norm_pre(x)

            # Get attention from last block
            for i, blk in enumerate(self.model.vit.blocks):
                if i == len(self.model.vit.blocks) - 1:
                    # Last block - get attention
                    x_norm = self.model.vit.blocks[i].norm1(x)
                    attn = self.model.vit.blocks[i].attn(x_norm)
                    # Extract attention weights (simplified - using output as proxy)
                    # For full attention, we'd need to modify the attention module
                    break
                x = blk(x)

            # Use a simpler approach: gradient-based
            input_tensor.requires_grad = True
            output = self.model(input_tensor)
            loss = output[0, target_class]
            loss.backward()

            # Get gradients
            gradients = input_tensor.grad.data
            gradients = gradients[0]  # Remove batch dimension

            # Global average pooling of gradients
            weights = gradients.mean(dim=(1, 2))  # Average over spatial dimensions

            # Create CAM by weighting input
            cam = (input_tensor[0] * weights.view(-1, 1, 1)).sum(dim=0)
            cam = F.relu(cam)

            # Normalize
            cam = cam.cpu().numpy()
            cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)

            return cam

    def __call__(self, input_tensor, targets=None):
        """
        Compatibility with pytorch_grad_cam interface
        """
        if targets is not None:
            target_class = targets[0].category if hasattr(targets[0], 'category') else targets[0]
        else:
            target_class = None

        cam = self.generate_cam(input_tensor, target_class)
        # Reshape to match expected format [batch, height, width]
        return np.expand_dims(cam, axis=0)

# Initialize Grad-CAM
try:
    # Try standard GradCAM first
    target_layers = [model.vit.blocks[-1]]
    cam = GradCAM(model=model, target_layers=target_layers)
    print("Standard Grad-CAM initialized!")
except Exception as e:
    print(f"Standard Grad-CAM failed: {e}")
    print("Using custom ViT Grad-CAM...")
    cam = ViTGradCAM(model, device)
    print("Custom ViT Grad-CAM initialized!")

def generate_gradcam(model, image_path, transform, device, class_idx=None, cam_obj=None):
    """
    Generate Grad-CAM heatmap for a given image
    """
    # Load and preprocess image
    image = Image.open(image_path).convert('RGB')
    original_image = np.array(image)
    original_image = cv2.resize(original_image, (224, 224))
    original_image_norm = original_image.astype(np.float32) / 255.0

    # Transform image
    input_tensor = transform(image).unsqueeze(0).to(device)

    # Get prediction first
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)
        probs = torch.softmax(output, dim=1)
        _, predicted = torch.max(output, 1)
        if class_idx is None:
            class_idx = predicted.item()

    # Generate CAM
    if cam_obj is None:
        cam_obj = cam

    if hasattr(cam_obj, 'generate_cam'):
        # Custom implementation
        grayscale_cam = cam_obj.generate_cam(input_tensor, class_idx)
        grayscale_cam = cv2.resize(grayscale_cam, (224, 224))
    else:
        # pytorch_grad_cam
        try:
            targets = [ClassifierOutputTarget(class_idx)]
            grayscale_cam = cam_obj(input_tensor=input_tensor, targets=targets)
            grayscale_cam = grayscale_cam[0]
        except:
            # Fallback
            grayscale_cam = np.ones((224, 224)) * 0.5

    # Overlay on image
    visualization = show_cam_on_image(original_image_norm, grayscale_cam, use_rgb=True)

    return visualization, grayscale_cam

print("Grad-CAM visualization functions ready!")

def evaluate_model(model, test_loader, device):
    """
    Comprehensive model evaluation with all metrics
    """
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Evaluating"):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_probs = np.array(all_probs)
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)

    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)

    # AUC-ROC
    try:
        if len(np.unique(all_labels)) > 1:
            auc_score = roc_auc_score(all_labels, all_probs[:, 1])
        else:
            auc_score = 0.0
    except:
        auc_score = 0.0

    # Confusion matrix
    cm = confusion_matrix(all_labels, all_preds)

    # Print metrics
    print(f"\n{'='*50}")
    print(f"           MODEL EVALUATION RESULTS")
    print(f"{'='*50}")
    print(f"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Precision: {precision:.4f} ({precision*100:.2f}%)")
    print(f"Recall:    {recall:.4f} ({recall*100:.2f}%)")
    print(f"F1-Score:  {f1:.4f} ({f1*100:.2f}%)")
    print(f"AUC-ROC:   {auc_score:.4f} ({auc_score*100:.2f}%)")
    print(f"{'='*50}\n")

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc_score,
        'confusion_matrix': cm,
        'predictions': all_preds,
        'labels': all_labels,
        'probabilities': all_probs
    }

def plot_evaluation_metrics(results):
    """
    Plot comprehensive evaluation metrics
    """
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    # 1. Confusion Matrix
    ax1 = fig.add_subplot(gs[0, 0])
    cm = results['confusion_matrix']
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal', 'Cancer'],
                yticklabels=['Normal', 'Cancer'],
                cbar_kws={'label': 'Count'})
    ax1.set_title('Confusion Matrix', fontsize=14, fontweight='bold')
    ax1.set_ylabel('True Label', fontsize=12)
    ax1.set_xlabel('Predicted Label', fontsize=12)

    # 2. ROC Curve
    ax2 = fig.add_subplot(gs[0, 1])
    fpr, tpr, thresholds = roc_curve(results['labels'], results['probabilities'][:, 1])
    roc_auc = results['auc']
    ax2.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROC curve (AUC = {roc_auc:.3f})')
    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('False Positive Rate', fontsize=12)
    ax2.set_ylabel('True Positive Rate', fontsize=12)
    ax2.set_title('ROC Curve', fontsize=14, fontweight='bold')
    ax2.legend(loc="lower right")
    ax2.grid(True, alpha=0.3)

    # 3. Metrics Bar Chart
    ax3 = fig.add_subplot(gs[0, 2])
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
    values = [results['accuracy'], results['precision'],
              results['recall'], results['f1'], results['auc']]
    colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#f39c12']
    bars = ax3.bar(metrics, values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    ax3.set_ylim([0, 1.1])
    ax3.set_ylabel('Score', fontsize=12)
    ax3.set_title('Performance Metrics', fontsize=14, fontweight='bold')
    ax3.grid(True, alpha=0.3, axis='y')

    # Add value labels on bars
    for bar, val in zip(bars, values):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')

    # 4. Metrics Summary Table
    ax4 = fig.add_subplot(gs[1, :])
    ax4.axis('tight')
    ax4.axis('off')

    table_data = [
        ['Metric', 'Value', 'Percentage'],
        ['Accuracy', f"{results['accuracy']:.4f}", f"{results['accuracy']*100:.2f}%"],
        ['Precision', f"{results['precision']:.4f}", f"{results['precision']*100:.2f}%"],
        ['Recall (Sensitivity)', f"{results['recall']:.4f}", f"{results['recall']*100:.2f}%"],
        ['F1-Score', f"{results['f1']:.4f}", f"{results['f1']*100:.2f}%"],
        ['AUC-ROC', f"{results['auc']:.4f}", f"{results['auc']*100:.2f}%"]
    ]

    table = ax4.table(cellText=table_data[1:], colLabels=table_data[0],
                      cellLoc='center', loc='center',
                      colWidths=[0.3, 0.2, 0.2])
    table.auto_set_font_size(False)
    table.set_fontsize(12)
    table.scale(1.2, 2)

    # Style the header
    for i in range(3):
        table[(0, i)].set_facecolor('#34495e')
        table[(0, i)].set_text_props(weight='bold', color='white')

    # Style the data rows
    for i in range(1, len(table_data)):
        for j in range(3):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#ecf0f1')

    ax4.set_title('Detailed Evaluation Metrics', fontsize=16, fontweight='bold', pad=20)

    # 5. Prediction Distribution
    ax5 = fig.add_subplot(gs[2, 0])
    pred_counts = pd.Series(results['predictions']).value_counts().sort_index()
    ax5.bar(['Normal', 'Cancer'], pred_counts.values, color=['#3498db', '#e74c3c'],
            alpha=0.8, edgecolor='black', linewidth=1.5)
    ax5.set_ylabel('Count', fontsize=12)
    ax5.set_title('Prediction Distribution', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    for i, v in enumerate(pred_counts.values):
        ax5.text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')

    # 6. True Label Distribution
    ax6 = fig.add_subplot(gs[2, 1])
    true_counts = pd.Series(results['labels']).value_counts().sort_index()
    ax6.bar(['Normal', 'Cancer'], true_counts.values, color=['#2ecc71', '#e74c3c'],
            alpha=0.8, edgecolor='black', linewidth=1.5)
    ax6.set_ylabel('Count', fontsize=12)
    ax6.set_title('True Label Distribution', fontsize=14, fontweight='bold')
    ax6.grid(True, alpha=0.3, axis='y')
    for i, v in enumerate(true_counts.values):
        ax6.text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')

    # 7. Confidence Score Distribution
    ax7 = fig.add_subplot(gs[2, 2])
    confidences = np.max(results['probabilities'], axis=1)
    ax7.hist(confidences, bins=30, color='#9b59b6', alpha=0.7, edgecolor='black')
    ax7.set_xlabel('Confidence Score', fontsize=12)
    ax7.set_ylabel('Frequency', fontsize=12)
    ax7.set_title('Confidence Score Distribution', fontsize=14, fontweight='bold')
    ax7.axvline(np.mean(confidences), color='red', linestyle='--',
                linewidth=2, label=f'Mean: {np.mean(confidences):.3f}')
    ax7.legend()
    ax7.grid(True, alpha=0.3, axis='y')

    plt.suptitle('Comprehensive Model Evaluation Dashboard',
                 fontsize=18, fontweight='bold', y=0.995)
    plt.show()

    return fig

print("Enhanced evaluation functions ready!")

def predict_and_visualize_complete(model, image_path, transform, device, cam_obj=None):
    """
    Complete prediction visualization with all requested outputs
    """
    model.eval()

    # Load and preprocess image
    image = Image.open(image_path).convert('RGB')
    original_image = np.array(image)
    original_image_resized = cv2.resize(original_image, (224, 224))
    original_image_norm = original_image_resized.astype(np.float32) / 255.0

    input_tensor = transform(image).unsqueeze(0).to(device)

    # Get prediction
    with torch.no_grad():
        output = model(input_tensor)
        probs = torch.softmax(output, dim=1)
        _, predicted = torch.max(output, 1)
        confidence = probs[0][predicted.item()].item()

    # Generate Grad-CAM
    if cam_obj is None:
        cam_obj = cam

    if hasattr(cam_obj, 'generate_cam'):
        grayscale_cam = cam_obj.generate_cam(input_tensor, predicted.item())
        grayscale_cam = cv2.resize(grayscale_cam, (224, 224))
    else:
        try:
            targets = [ClassifierOutputTarget(predicted.item())]
            grayscale_cam = cam_obj(input_tensor=input_tensor, targets=targets)
            grayscale_cam = grayscale_cam[0]
        except:
            grayscale_cam = np.ones((224, 224)) * 0.5

    visualization = show_cam_on_image(original_image_norm, grayscale_cam, use_rgb=True)

    # Create comprehensive visualization
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    # 1. Original Image
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.imshow(original_image_norm)
    ax1.set_title('Original CT Scan Image', fontsize=14, fontweight='bold', pad=10)
    ax1.axis('off')

    # 2. Prediction Result with Confidence
    ax2 = fig.add_subplot(gs[0, 1])
    ax2.imshow(original_image_norm)
    prediction_text = "CANCER DETECTED" if predicted.item() == 1 else "NORMAL"
    color = '#e74c3c' if predicted.item() == 1 else '#2ecc71'
    ax2.text(0.5, 0.95, prediction_text,
            transform=ax2.transAxes, fontsize=20, fontweight='bold',
            ha='center', va='top', color=color,
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor=color, linewidth=3))
    ax2.text(0.5, 0.05, f'Confidence: {confidence:.2%}',
            transform=ax2.transAxes, fontsize=16, fontweight='bold',
            ha='center', va='bottom', color='black',
            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
    ax2.set_title('Prediction Result', fontsize=14, fontweight='bold', pad=10)
    ax2.axis('off')

    # 3. Grad-CAM Heatmap Overlay
    ax3 = fig.add_subplot(gs[0, 2])
    ax3.imshow(visualization)
    ax3.set_title('Grad-CAM Heatmap Overlay', fontsize=14, fontweight='bold', pad=10)
    ax3.axis('off')

    # 4. Raw Grad-CAM Heatmap
    ax4 = fig.add_subplot(gs[1, 0])
    im4 = ax4.imshow(grayscale_cam, cmap='jet', interpolation='bilinear')
    ax4.set_title('Grad-CAM Heatmap (Raw)', fontsize=14, fontweight='bold', pad=10)
    ax4.axis('off')
    plt.colorbar(im4, ax=ax4, fraction=0.046, pad=0.04)

    # 5. Probability Distribution
    ax5 = fig.add_subplot(gs[1, 1])
    classes = ['Normal', 'Cancer']
    probs_np = probs[0].cpu().numpy()
    colors_bar = ['#2ecc71' if i == 0 else '#e74c3c' for i in range(len(classes))]
    bars = ax5.bar(classes, probs_np, color=colors_bar, alpha=0.8, edgecolor='black', linewidth=2)
    ax5.set_ylim([0, 1.1])
    ax5.set_ylabel('Probability', fontsize=12, fontweight='bold')
    ax5.set_title('Class Probabilities', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')

    # Add value labels
    for bar, prob in zip(bars, probs_np):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{prob:.3f}\n({prob*100:.1f}%)',
                ha='center', va='bottom', fontweight='bold', fontsize=11)

    # Highlight predicted class
    bars[predicted.item()].set_edgecolor('yellow')
    bars[predicted.item()].set_linewidth(4)

    # 6. Attention Visualization
    ax6 = fig.add_subplot(gs[1, 2])
    attention_overlay = cv2.applyColorMap((grayscale_cam * 255).astype(np.uint8), cv2.COLORMAP_JET)
    attention_overlay = cv2.cvtColor(attention_overlay, cv2.COLOR_BGR2RGB)
    attention_blended = cv2.addWeighted(
        (original_image_norm * 255).astype(np.uint8), 0.6,
        attention_overlay, 0.4, 0
    ) / 255.0
    ax6.imshow(attention_blended)
    ax6.set_title('Attention Visualization', fontsize=14, fontweight='bold', pad=10)
    ax6.axis('off')

    # 7. Segmentation-like Mask
    ax7 = fig.add_subplot(gs[2, 0])
    threshold = 0.5
    mask = (grayscale_cam > threshold).astype(np.float32)
    masked_image = original_image_norm.copy()
    masked_image[mask == 0] = masked_image[mask == 0] * 0.3
    ax7.imshow(masked_image)
    ax7.set_title(f'Segmentation Mask (Threshold > {threshold})',
                 fontsize=14, fontweight='bold', pad=10)
    ax7.axis('off')

    # 8. Detailed Prediction Info
    ax8 = fig.add_subplot(gs[2, 1:])
    ax8.axis('off')

    info_text = f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              PREDICTION DETAILS                               â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  Prediction:        {prediction_text:20s}                    â•‘
    â•‘  Confidence Score:  {confidence:.4f} ({confidence*100:.2f}%)                    â•‘
    â•‘  Normal Probability: {probs_np[0]:.4f} ({probs_np[0]*100:.2f}%)                    â•‘
    â•‘  Cancer Probability: {probs_np[1]:.4f} ({probs_np[1]*100:.2f}%)                    â•‘
    â•‘                                                              â•‘
    â•‘  Interpretation:                                             â•‘
    â•‘  {'âš ï¸ HIGH RISK - Further medical evaluation recommended' if predicted.item() == 1 else 'âœ… LOW RISK - No signs of cancer detected'}  â•‘
    â•‘                                                              â•‘
    â•‘  Note: This is an AI-assisted diagnosis tool.                â•‘
    â•‘        Always consult with a qualified radiologist.          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """

    ax8.text(0.5, 0.5, info_text, transform=ax8.transAxes,
            fontsize=12, fontfamily='monospace',
            ha='center', va='center',
            bbox=dict(boxstyle='round', facecolor='#ecf0f1',
                     edgecolor='#34495e', linewidth=2))

    plt.suptitle('Complete Lung Cancer Detection Analysis',
                 fontsize=20, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.show()

    return {
        'prediction': 'Cancer' if predicted.item() == 1 else 'Normal',
        'confidence': confidence,
        'probabilities': probs_np,
        'gradcam': grayscale_cam,
        'visualization': visualization
    }

print("Complete prediction visualization function ready!")

def visualize_batch_predictions(model, test_loader, device, num_samples=6, cam_obj=None):
    """
    Visualize multiple predictions with all features
    """
    model.eval()

    fig = plt.figure(figsize=(24, 4*num_samples))
    gs = fig.add_gridspec(num_samples, 4, hspace=0.4, wspace=0.3)

    sample_count = 0

    if cam_obj is None:
        cam_obj = cam

    with torch.no_grad():
        for images, labels in test_loader:
            if sample_count >= num_samples:
                break

            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)

            for i in range(min(images.size(0), num_samples - sample_count)):
                if sample_count >= num_samples:
                    break

                # Get image
                image_np = images[i].cpu().permute(1, 2, 0).numpy()
                mean = np.array([0.485, 0.456, 0.406])
                std = np.array([0.229, 0.224, 0.225])
                image_np = image_np * std + mean
                image_np = np.clip(image_np, 0, 1)

                # Generate Grad-CAM
                if hasattr(cam_obj, 'generate_cam'):
                    grayscale_cam = cam_obj.generate_cam(images[i:i+1], predicted[i].item())
                    grayscale_cam = cv2.resize(grayscale_cam, (224, 224))
                else:
                    try:
                        targets = [ClassifierOutputTarget(predicted[i].item())]
                        grayscale_cam = cam_obj(input_tensor=images[i:i+1], targets=targets)
                        grayscale_cam = grayscale_cam[0]
                    except:
                        grayscale_cam = np.ones((224, 224)) * 0.5

                visualization = show_cam_on_image(image_np, grayscale_cam, use_rgb=True)

                # Original
                ax1 = fig.add_subplot(gs[sample_count, 0])
                ax1.imshow(image_np)
                ax1.set_title(f'Sample {sample_count+1}\nTrue: {"Cancer" if labels[i].item() == 1 else "Normal"}',
                             fontsize=12, fontweight='bold')
                ax1.axis('off')

                # Prediction
                ax2 = fig.add_subplot(gs[sample_count, 1])
                ax2.imshow(image_np)
                pred_text = "CANCER" if predicted[i].item() == 1 else "NORMAL"
                color = '#e74c3c' if predicted[i].item() == 1 else '#2ecc71'
                correct = "âœ“" if predicted[i].item() == labels[i].item() else "âœ—"
                ax2.text(0.5, 0.95, f'{pred_text} {correct}',
                        transform=ax2.transAxes, fontsize=16, fontweight='bold',
                        ha='center', va='top', color=color,
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9,
                                 edgecolor=color, linewidth=2))
                ax2.text(0.5, 0.05, f'Conf: {probs[i][predicted[i]].item():.2%}',
                        transform=ax2.transAxes, fontsize=12, fontweight='bold',
                        ha='center', va='bottom', color='black',
                        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
                ax2.set_title('Prediction', fontsize=12, fontweight='bold')
                ax2.axis('off')

                # Grad-CAM
                ax3 = fig.add_subplot(gs[sample_count, 2])
                ax3.imshow(visualization)
                ax3.set_title('Grad-CAM Overlay', fontsize=12, fontweight='bold')
                ax3.axis('off')

                # Heatmap
                ax4 = fig.add_subplot(gs[sample_count, 3])
                im = ax4.imshow(grayscale_cam, cmap='jet', interpolation='bilinear')
                ax4.set_title('Attention Heatmap', fontsize=12, fontweight='bold')
                ax4.axis('off')
                plt.colorbar(im, ax=ax4, fraction=0.046, pad=0.04)

                sample_count += 1

    plt.suptitle('Batch Prediction Results with Explainability',
                 fontsize=18, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.show()

print("Batch visualization function ready!")

# ============================================
# COMPLETE PIPELINE EXECUTION
# ============================================

print("\n" + "="*60)
print("  COMPLETE LUNG CANCER DETECTION PIPELINE")
print("="*60 + "\n")

# 1. Model Evaluation
print("Step 1: Evaluating model performance...")
results = evaluate_model(model, test_loader, device)

# 2. Comprehensive Metrics Dashboard
print("\nStep 2: Generating evaluation dashboard...")
plot_evaluation_metrics(results)

# 3. Batch Predictions
print("\nStep 3: Visualizing batch predictions...")
visualize_batch_predictions(model, test_loader, device, num_samples=6)

# 4. Single Image Prediction (using first test image as example)
print("\nStep 4: Analyzing sample image...")
sample_image_path = test_paths[0]  # Use first test image
if os.path.exists(sample_image_path):
    prediction_result = predict_and_visualize_complete(
        model, sample_image_path, val_transform, device
    )
    print(f"\nâœ“ Prediction: {prediction_result['prediction']}")
    print(f"âœ“ Confidence: {prediction_result['confidence']:.2%}")
else:
    print(f"Sample image not found: {sample_image_path}")

print("\n" + "="*60)
print("  PIPELINE COMPLETE!")
print("="*60)

# ============================================
# EXPLAINABLE AI (XAI) DEMONSTRATION
# ============================================
# This cell demonstrates how Grad-CAM explains model predictions

def demonstrate_explainable_ai(model, test_loader, device, num_samples=3, cam_obj=None):
    """
    Comprehensive demonstration of Explainable AI using Grad-CAM
    Shows how the model makes decisions by highlighting important regions
    """
    print("="*70)
    print("  EXPLAINABLE AI (XAI) DEMONSTRATION - Grad-CAM")
    print("="*70)
    print("\nGrad-CAM (Gradient-weighted Class Activation Mapping) is used to:")
    print("  1. Visualize which regions of the CT scan influenced the prediction")
    print("  2. Provide transparency in the AI decision-making process")
    print("  3. Help radiologists understand and trust the model's predictions")
    print("  4. Identify potential areas of concern in the medical images")
    print("\n" + "="*70 + "\n")

    model.eval()

    if cam_obj is None:
        cam_obj = cam

    fig = plt.figure(figsize=(20, 6*num_samples))
    gs = fig.add_gridspec(num_samples, 5, hspace=0.4, wspace=0.3)

    sample_count = 0

    with torch.no_grad():
        for images, labels in test_loader:
            if sample_count >= num_samples:
                break

            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs, 1)

            for i in range(min(images.size(0), num_samples - sample_count)):
                if sample_count >= num_samples:
                    break

                # Get image
                image_np = images[i].cpu().permute(1, 2, 0).numpy()
                mean = np.array([0.485, 0.456, 0.406])
                std = np.array([0.229, 0.224, 0.225])
                image_np = image_np * std + mean
                image_np = np.clip(image_np, 0, 1)

                # Generate Grad-CAM for predicted class
                if hasattr(cam_obj, 'generate_cam'):
                    grayscale_cam = cam_obj.generate_cam(images[i:i+1], predicted[i].item())
                    grayscale_cam = cv2.resize(grayscale_cam, (224, 224))
                else:
                    try:
                        targets = [ClassifierOutputTarget(predicted[i].item())]
                        grayscale_cam = cam_obj(input_tensor=images[i:i+1], targets=targets)
                        grayscale_cam = grayscale_cam[0]
                    except:
                        grayscale_cam = np.ones((224, 224)) * 0.5

                # Create visualizations
                visualization = show_cam_on_image(image_np, grayscale_cam, use_rgb=True)

                # 1. Original Image
                ax1 = fig.add_subplot(gs[sample_count, 0])
                ax1.imshow(image_np)
                ax1.set_title('Original CT Scan', fontsize=12, fontweight='bold')
                ax1.axis('off')

                # 2. Prediction with Confidence
                ax2 = fig.add_subplot(gs[sample_count, 1])
                ax2.imshow(image_np)
                pred_text = "CANCER" if predicted[i].item() == 1 else "NORMAL"
                true_text = "CANCER" if labels[i].item() == 1 else "NORMAL"
                color = '#e74c3c' if predicted[i].item() == 1 else '#2ecc71'
                correct = "âœ“ CORRECT" if predicted[i].item() == labels[i].item() else "âœ— INCORRECT"

                ax2.text(0.5, 0.95, f'Predicted: {pred_text}',
                        transform=ax2.transAxes, fontsize=14, fontweight='bold',
                        ha='center', va='top', color=color,
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9,
                                 edgecolor=color, linewidth=2))
                ax2.text(0.5, 0.75, f'True: {true_text}',
                        transform=ax2.transAxes, fontsize=12,
                        ha='center', va='top', color='black',
                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
                ax2.text(0.5, 0.55, correct,
                        transform=ax2.transAxes, fontsize=12, fontweight='bold',
                        ha='center', va='top',
                        color='green' if predicted[i].item() == labels[i].item() else 'red',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
                ax2.text(0.5, 0.05, f'Confidence: {probs[i][predicted[i]].item():.2%}',
                        transform=ax2.transAxes, fontsize=11, fontweight='bold',
                        ha='center', va='bottom', color='black',
                        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
                ax2.set_title('AI Prediction', fontsize=12, fontweight='bold')
                ax2.axis('off')

                # 3. Grad-CAM Overlay (EXPLAINABLE AI - Main Output)
                ax3 = fig.add_subplot(gs[sample_count, 2])
                ax3.imshow(visualization)
                ax3.set_title('Grad-CAM Heatmap Overlay\n(Explainable AI)',
                             fontsize=12, fontweight='bold', color='darkblue')
                ax3.axis('off')
                # Add explanation text
                ax3.text(0.5, -0.1, 'Red/Yellow = High Attention\nBlue = Low Attention',
                        transform=ax3.transAxes, fontsize=9, ha='center',
                        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))

                # 4. Raw Heatmap (EXPLAINABLE AI - Attention Map)
                ax4 = fig.add_subplot(gs[sample_count, 3])
                im4 = ax4.imshow(grayscale_cam, cmap='jet', interpolation='bilinear')
                ax4.set_title('Attention Heatmap\n(Raw Grad-CAM)',
                             fontsize=12, fontweight='bold', color='darkblue')
                ax4.axis('off')
                plt.colorbar(im4, ax=ax4, fraction=0.046, pad=0.04, label='Attention')

                # 5. Segmentation Mask (EXPLAINABLE AI - Thresholded Attention)
                ax5 = fig.add_subplot(gs[sample_count, 4])
                threshold = 0.5
                mask = (grayscale_cam > threshold).astype(np.float32)
                masked_image = image_np.copy()
                # Highlight attention regions
                masked_image[mask == 1] = masked_image[mask == 1] * 1.2  # Brighten attention areas
                masked_image[mask == 0] = masked_image[mask == 0] * 0.4  # Dim non-attention areas
                masked_image = np.clip(masked_image, 0, 1)
                ax5.imshow(masked_image)
                ax5.set_title(f'Segmentation Mask\n(Threshold > {threshold})',
                             fontsize=12, fontweight='bold', color='darkblue')
                ax5.axis('off')
                ax5.text(0.5, -0.1, 'Bright = Model Focus Area',
                        transform=ax5.transAxes, fontsize=9, ha='center',
                        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

                sample_count += 1

    plt.suptitle('Explainable AI (XAI) - How the Model Makes Decisions',
                 fontsize=18, fontweight='bold', y=0.995, color='darkblue')
    plt.tight_layout()
    plt.show()

    print("\n" + "="*70)
    print("  EXPLAINABLE AI INTERPRETATION GUIDE")
    print("="*70)
    print("\n1. GRAD-CAM HEATMAP OVERLAY:")
    print("   - Shows which regions the model focused on when making the prediction")
    print("   - Red/Yellow areas = High importance (model's attention)")
    print("   - Blue areas = Low importance")
    print("\n2. ATTENTION HEATMAP:")
    print("   - Raw attention scores from the model")
    print("   - Higher values indicate regions that influenced the decision")
    print("\n3. SEGMENTATION MASK:")
    print("   - Highlights regions above a threshold attention value")
    print("   - Helps identify specific areas of concern")
    print("\n4. CLINICAL SIGNIFICANCE:")
    print("   - Radiologists can verify if highlighted regions match known pathology")
    print("   - Builds trust in AI predictions through transparency")
    print("   - Enables validation of model reasoning")
    print("="*70 + "\n")

# Run the demonstration
print("Running Explainable AI demonstration...")
demonstrate_explainable_ai(model, test_loader, device, num_samples=3)

# Filter out rows where images don't exist and labels are missing
df_labeled = df[(df['image_exists'] == True) & (df['label'].notna())].copy()

print(f"Valid labeled images: {len(df_labeled)} / {len(df)}")

if len(df_labeled) == 0:
    print("ERROR: No valid labeled images found!")
    print("Please check the dataset structure.")
else:
    # Get image paths and labels
    image_paths = df_labeled['image_path'].values
    labels = df_labeled['label'].astype(int).values

    print(f"\nLabel distribution:")
    unique, counts = np.unique(labels, return_counts=True)
    for u, c in zip(unique, counts):
        label_name = 'Normal' if u == 0 else 'Cancer'
        print(f"  {label_name} (Label {u}): {c} images")

    # Check if we have enough samples for stratified split
    min_samples = min(counts) if len(counts) > 1 else counts[0]

    if min_samples < 2:
        print("\nWarning: Not enough samples for stratified split. Using random split.")
        stratify = None
    else:
        stratify = labels

    # Split data into train, validation, and test sets
    # First split: train (70%) and temp (30%)
    train_paths, temp_paths, train_labels, temp_labels = train_test_split(
        image_paths, labels, test_size=0.3, random_state=42, stratify=stratify
    )

    # Second split: validation (15%) and test (15%)
    if stratify is not None:
        temp_stratify = temp_labels
    else:
        temp_stratify = None

    val_paths, test_paths, val_labels, test_labels = train_test_split(
        temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_stratify
    )

    print(f"\nData split:")
    print(f"  Train: {len(train_paths)} images")
    print(f"  Validation: {len(val_paths)} images")
    print(f"  Test: {len(test_paths)} images")

    # Show label distribution in each split
    print(f"\nTrain label distribution:")
    unique_train, counts_train = np.unique(train_labels, return_counts=True)
    for u, c in zip(unique_train, counts_train):
        label_name = 'Normal' if u == 0 else 'Cancer'
        print(f"  {label_name}: {c}")

    # Create datasets
    train_dataset = LungCancerDataset(train_paths, train_labels, transform=train_transform)
    val_dataset = LungCancerDataset(val_paths, val_labels, transform=val_transform)
    test_dataset = LungCancerDataset(test_paths, test_labels, transform=val_transform)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

    print("\nâœ“ Data loaders created successfully!")
    print("You can now proceed to training (Cell 10).")

# ============================================
# DATASET SUMMARY VISUALIZATION
# ============================================

print("\n" + "="*70)
print("  DATASET SUMMARY - IMAGE STATISTICS")
print("="*70)

# Calculate statistics
total_images = len(df_labeled)
normal_count = (df_labeled['label'] == 0).sum()
cancer_count = (df_labeled['label'] == 1).sum()

train_count = len(train_loader.dataset)
val_count = len(val_loader.dataset)
test_count = len(test_loader.dataset)

# Get label distribution in each split
train_labels = [train_loader.dataset[i][1] for i in range(len(train_loader.dataset))]
val_labels = [val_loader.dataset[i][1] for i in range(len(val_loader.dataset))]
test_labels = [test_loader.dataset[i][1] for i in range(len(test_loader.dataset))]

train_normal = train_labels.count(0)
train_cancer = train_labels.count(1)
val_normal = val_labels.count(0)
val_cancer = val_labels.count(1)
test_normal = test_labels.count(0)
test_cancer = test_labels.count(1)

# Display summary
print(f"\nðŸ“Š OVERALL DATASET:")
print(f"   Total Images: {total_images}")
print(f"   â”œâ”€ Normal: {normal_count} ({normal_count/total_images*100:.1f}%)")
print(f"   â””â”€ Cancer: {cancer_count} ({cancer_count/total_images*100:.1f}%)")

print(f"\nðŸ“ DATA SPLIT:")
print(f"   Training Set:   {train_count:3d} images ({train_count/total_images*100:.1f}%)")
print(f"   â”œâ”€ Normal: {train_normal:3d} ({train_normal/train_count*100:.1f}%)")
print(f"   â””â”€ Cancer: {train_cancer:3d} ({train_cancer/train_count*100:.1f}%)")

print(f"\n   Validation Set: {val_count:3d} images ({val_count/total_images*100:.1f}%)")
print(f"   â”œâ”€ Normal: {val_normal:3d} ({val_normal/val_count*100:.1f}%)")
print(f"   â””â”€ Cancer: {val_cancer:3d} ({val_cancer/val_count*100:.1f}%)")

print(f"\n   Test Set:       {test_count:3d} images ({test_count/total_images*100:.1f}%)")
print(f"   â”œâ”€ Normal: {test_normal:3d} ({test_normal/test_count*100:.1f}%)")
print(f"   â””â”€ Cancer: {test_cancer:3d} ({test_cancer/test_count*100:.1f}%)")

print(f"\nâœ… Total images used: {train_count + val_count + test_count} / {total_images}")
print("="*70)

# Create visualizations
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. Overall label distribution
ax1 = axes[0]
labels_pie = ['Normal', 'Cancer']
sizes_pie = [normal_count, cancer_count]
colors_pie = ['#2ecc71', '#e74c3c']
ax1.pie(sizes_pie, labels=labels_pie, colors=colors_pie, autopct='%1.1f%%',
        startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
ax1.set_title('Overall Label Distribution', fontsize=14, fontweight='bold', pad=15)

# 2. Dataset split
ax2 = axes[1]
split_labels = ['Train', 'Validation', 'Test']
split_sizes = [train_count, val_count, test_count]
split_colors = ['#3498db', '#f39c12', '#e74c3c']
bars = ax2.bar(split_labels, split_sizes, color=split_colors, alpha=0.8,
               edgecolor='black', linewidth=2)
ax2.set_ylabel('Number of Images', fontsize=12, fontweight='bold')
ax2.set_title('Dataset Split Distribution', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bar, size in zip(bars, split_sizes):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + max(split_sizes)*0.01,
            f'{size}', ha='center', va='bottom', fontweight='bold', fontsize=12)

# 3. Label distribution in each split
ax3 = axes[2]
x = np.arange(3)
width = 0.35
normal_counts = [train_normal, val_normal, test_normal]
cancer_counts = [train_cancer, val_cancer, test_cancer]

bars1 = ax3.bar(x - width/2, normal_counts, width, label='Normal',
                color='#2ecc71', alpha=0.8, edgecolor='black', linewidth=1.5)
bars2 = ax3.bar(x + width/2, cancer_counts, width, label='Cancer',
                color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5)

ax3.set_ylabel('Number of Images', fontsize=12, fontweight='bold')
ax3.set_title('Label Distribution by Split', fontsize=14, fontweight='bold')
ax3.set_xticks(x)
ax3.set_xticklabels(['Train', 'Validation', 'Test'])
# Fixed: Set legend properties correctly
legend = ax3.legend(fontsize=11)
legend.get_frame().set_facecolor('white')
legend.get_frame().set_alpha(0.9)
# Set font weight for legend text
for text in legend.get_texts():
    text.set_fontweight('bold')
ax3.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax3.text(bar.get_x() + bar.get_width()/2., height + 2,
                    f'{int(height)}', ha='center', va='bottom',
                    fontweight='bold', fontsize=10)

plt.tight_layout()
plt.show()

print("\nâœ… Dataset is ready for training!")
print("="*70 + "\n")